{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing ./logs-imagnet_64_baseline/hparams-imagnet_64_baseline.cfg! Resuming run using primary parameters!\n"
     ]
    }
   ],
   "source": [
    "from hparams import HParams\n",
    "hparams = HParams('.', name=\"efficient_vdvae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 17:32:53.638767: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 17:32:54.944453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from numpy.random import seed\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils import assert_CUDA_and_hparams_gpus_are_equal, create_checkpoint_manager_and_load_if_exists, \\\n",
    "        get_logdir, get_variate_masks, transpose_dicts\n",
    "from data.generic_data_loader import synth_generic_data, encode_generic_data, stats_generic_data\n",
    "from data.cifar10_data_loader import synth_cifar_data, encode_cifar_data, stats_cifar_data\n",
    "from data.imagenet_data_loader import synth_imagenet_data, encode_imagenet_data, stats_imagenet_data\n",
    "from data.mnist_data_loader import synth_mnist_data, encode_mnist_data, stats_mnist_data\n",
    "from model.def_model import UniversalAutoEncoder\n",
    "from model.model import reconstruction_step, generation_step, encode_step\n",
    "from model.losses import StructureSimilarityIndexMap\n",
    "from utils import temperature_functions\n",
    "from model.div_stats_utils import KLDivergenceStats\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Fix random seeds\n",
    "torch.manual_seed(hparams.run.seed)\n",
    "torch.manual_seed(hparams.run.seed)\n",
    "torch.cuda.manual_seed(hparams.run.seed)\n",
    "torch.cuda.manual_seed_all(hparams.run.seed)  # if you are using multi-GPU.\n",
    "seed(hparams.run.seed)  # Numpy module.\n",
    "random.seed(hparams.run.seed)  # Python random module.\n",
    "torch.manual_seed(hparams.run.seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to True.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.profiler.emit_nvtx at 0x7fcf146b01f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pickle.load(open('logs-imagnet_64_baseline/latents/encodings_temp_first100.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'latent_codes'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 64, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(embeddings['images'].values())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 22, 24, 25, 26, 27, 30, 31, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 79, 80, 81, 82])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['latent_codes']['image_1093444'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 1, 2, 2) 4\n",
      "1 (1, 1, 1, 2) 2\n",
      "2 (1, 1, 1, 2) 2\n",
      "3 (1, 1, 2, 2) 4\n",
      "4 (1, 1, 1, 2) 2\n",
      "5 (1, 1, 2, 2) 4\n",
      "6 (4, 4, 2, 2) 64\n",
      "7 (4, 4, 2, 2) 64\n",
      "8 (4, 4, 2, 2) 64\n",
      "9 (4, 4, 2, 2) 64\n",
      "10 (4, 4, 2, 2) 64\n",
      "11 (4, 4, 2, 2) 64\n",
      "12 (4, 4, 1, 2) 32\n",
      "14 (8, 8, 1, 2) 128\n",
      "15 (8, 8, 1, 2) 128\n",
      "16 (8, 8, 2, 2) 256\n",
      "17 (8, 8, 2, 2) 256\n",
      "18 (8, 8, 2, 2) 256\n",
      "19 (8, 8, 2, 2) 256\n",
      "20 (8, 8, 1, 2) 128\n",
      "22 (8, 8, 1, 2) 128\n",
      "24 (8, 8, 1, 2) 128\n",
      "25 (8, 8, 1, 2) 128\n",
      "26 (8, 8, 1, 2) 128\n",
      "27 (8, 8, 1, 2) 128\n",
      "30 (8, 8, 1, 2) 128\n",
      "31 (8, 8, 1, 2) 128\n",
      "36 (16, 16, 1, 2) 512\n",
      "37 (16, 16, 1, 2) 512\n",
      "38 (16, 16, 1, 2) 512\n",
      "39 (16, 16, 1, 2) 512\n",
      "40 (16, 16, 1, 2) 512\n",
      "41 (16, 16, 1, 2) 512\n",
      "42 (16, 16, 1, 2) 512\n",
      "43 (16, 16, 1, 2) 512\n",
      "44 (16, 16, 1, 2) 512\n",
      "45 (16, 16, 1, 2) 512\n",
      "46 (16, 16, 1, 2) 512\n",
      "47 (16, 16, 1, 2) 512\n",
      "48 (16, 16, 1, 2) 512\n",
      "49 (16, 16, 1, 2) 512\n",
      "50 (16, 16, 1, 2) 512\n",
      "51 (16, 16, 1, 2) 512\n",
      "52 (16, 16, 1, 2) 512\n",
      "53 (16, 16, 1, 2) 512\n",
      "56 (16, 16, 1, 2) 512\n",
      "58 (32, 32, 1, 2) 2048\n",
      "59 (32, 32, 1, 2) 2048\n",
      "60 (32, 32, 1, 2) 2048\n",
      "61 (32, 32, 1, 2) 2048\n",
      "62 (32, 32, 1, 2) 2048\n",
      "63 (32, 32, 1, 2) 2048\n",
      "64 (32, 32, 1, 2) 2048\n",
      "65 (32, 32, 1, 2) 2048\n",
      "66 (32, 32, 1, 2) 2048\n",
      "67 (32, 32, 1, 2) 2048\n",
      "68 (32, 32, 1, 2) 2048\n",
      "69 (32, 32, 1, 2) 2048\n",
      "70 (32, 32, 1, 2) 2048\n",
      "71 (32, 32, 1, 2) 2048\n",
      "72 (32, 32, 1, 2) 2048\n",
      "74 (64, 64, 1, 2) 8192\n",
      "76 (64, 64, 1, 2) 8192\n",
      "77 (64, 64, 1, 2) 8192\n",
      "79 (64, 64, 1, 2) 8192\n",
      "80 (64, 64, 1, 2) 8192\n",
      "81 (64, 64, 1, 2) 8192\n",
      "82 (64, 64, 1, 2) 8192\n"
     ]
    }
   ],
   "source": [
    "for i in embeddings['latent_codes']['image_1093444']:\n",
    "    print(i, embeddings['latent_codes']['image_1093444'][i].shape, embeddings['latent_codes']['image_1093444'][i].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100530"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 0\n",
    "for i in embeddings['latent_codes']['image_1093444']:\n",
    "    embedding_size += embeddings['latent_codes']['image_1093444'][i].size\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UniversalAutoEncoder()\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    ones = torch.ones((1, hparams.data.channels, hparams.data.target_res, hparams.data.target_res)).cuda(0)\n",
    "    _ = model(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint, checkpoint_path = create_checkpoint_manager_and_load_if_exists(rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA model is loaded\n",
      "./checkpoints-imagnet_64_baseline\n"
     ]
    }
   ],
   "source": [
    "if hparams.synthesis.load_ema_weights:\n",
    "    assert checkpoint['ema_model_state_dict'] is not None\n",
    "    model.load_state_dict(checkpoint['ema_model_state_dict'])\n",
    "    print('EMA model is loaded')\n",
    "else:\n",
    "    assert checkpoint['model_state_dict'] is not None\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Model Checkpoint is loaded')\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 1281167\n",
      "Path:  ../datasets/imagenet_64/train_data/\n"
     ]
    }
   ],
   "source": [
    "def encode_data():\n",
    "    if hparams.data.dataset_source in ['ffhq', 'celebAHQ', 'celebA', 'custom']:\n",
    "        return encode_generic_data()\n",
    "    elif hparams.data.dataset_source == 'cifar-10':\n",
    "        return encode_cifar_data()\n",
    "    elif hparams.data.dataset_source == 'binarized_mnist':\n",
    "        return encode_mnist_data()\n",
    "    elif hparams.data.dataset_source == 'imagenet':\n",
    "        return encode_imagenet_data()\n",
    "    else:\n",
    "        raise ValueError(f'Dataset {hparams.data.dataset_source} is not included.')\n",
    "data_loader = encode_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variate_masks(stats):\n",
    "    thresh = np.quantile(stats, 1 - 0.03)\n",
    "    return stats > thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84, 32), (84, 32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_stats = np.load('logs-imagnet_64_baseline/latents/div_stats.npy')\n",
    "variate_masks = get_variate_masks(div_stats)\n",
    "div_stats.shape, variate_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_distribution(dist_list, variate_mask):\n",
    "    \"\"\"\n",
    "    :param dist_list: n_layers, 2*  [ batch_size n_variates, H , W]\n",
    "    :return: Tensors  of shape batch_size, H, W ,n_variates, 2\n",
    "    H, W , n_variates will be different from each other in the list depending on which layer you are in.\n",
    "    \"\"\"\n",
    "    dist = torch.stack(dist_list, dim=0)  # 2, batch_size, n_variates, H ,W\n",
    "    dist = dist[:, :, variate_mask, :, :]  # Only take effective variates\n",
    "    dist = torch.permute(dist, (1, 3, 4, 2, 0))  # batch_size, H ,W ,n_variates (subset), 2\n",
    "    # dist = torch.unbind(dist, dim=0)  # Return a list of tensors of length batch_size\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10010 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10010 [00:10<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, (inputs, filenames) in enumerate(tqdm(data_loader)):\n",
    "        print(len(filenames), len(inputs))\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        predictions, posterior_dist_list, prior_kl_dist_list = model(inputs, variate_masks)\n",
    "\n",
    "        # If the mask states all variables of a layer are not effective we don't collect any latents from that layer\n",
    "        # n_layers , batch_size, [H, W, n_variates, 2]\n",
    "        dist_dict = {}\n",
    "        for i, (dist_list, variate_mask) in enumerate(zip(posterior_dist_list, variate_masks)):\n",
    "            if variate_mask.any():\n",
    "                x = reshape_distribution(dist_list, variate_mask).detach().cpu().numpy()\n",
    "                v = {name: xa for name, xa in zip(filenames, list(x))}\n",
    "                dist_dict[i] = v\n",
    "\n",
    "        if step == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 100, 64, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (32, 1, 1) 32\n",
      "1 (32, 1, 1) 32\n",
      "2 (32, 1, 1) 32\n",
      "3 (32, 1, 1) 32\n",
      "4 (32, 1, 1) 32\n",
      "5 (32, 1, 1) 32\n",
      "6 (32, 4, 4) 512\n",
      "7 (32, 4, 4) 512\n",
      "8 (32, 4, 4) 512\n",
      "9 (32, 4, 4) 512\n",
      "10 (32, 4, 4) 512\n",
      "11 (32, 4, 4) 512\n",
      "12 (32, 4, 4) 512\n",
      "13 (32, 8, 8) 2048\n",
      "14 (32, 8, 8) 2048\n",
      "15 (32, 8, 8) 2048\n",
      "16 (32, 8, 8) 2048\n",
      "17 (32, 8, 8) 2048\n",
      "18 (32, 8, 8) 2048\n",
      "19 (32, 8, 8) 2048\n",
      "20 (32, 8, 8) 2048\n",
      "21 (32, 8, 8) 2048\n",
      "22 (32, 8, 8) 2048\n",
      "23 (32, 8, 8) 2048\n",
      "24 (32, 8, 8) 2048\n",
      "25 (32, 8, 8) 2048\n",
      "26 (32, 8, 8) 2048\n",
      "27 (32, 8, 8) 2048\n",
      "28 (32, 8, 8) 2048\n",
      "29 (32, 8, 8) 2048\n",
      "30 (32, 8, 8) 2048\n",
      "31 (32, 8, 8) 2048\n",
      "32 (32, 16, 16) 8192\n",
      "33 (32, 16, 16) 8192\n",
      "34 (32, 16, 16) 8192\n",
      "35 (32, 16, 16) 8192\n",
      "36 (32, 16, 16) 8192\n",
      "37 (32, 16, 16) 8192\n",
      "38 (32, 16, 16) 8192\n",
      "39 (32, 16, 16) 8192\n",
      "40 (32, 16, 16) 8192\n",
      "41 (32, 16, 16) 8192\n",
      "42 (32, 16, 16) 8192\n",
      "43 (32, 16, 16) 8192\n",
      "44 (32, 16, 16) 8192\n",
      "45 (32, 16, 16) 8192\n",
      "46 (32, 16, 16) 8192\n",
      "47 (32, 16, 16) 8192\n",
      "48 (32, 16, 16) 8192\n",
      "49 (32, 16, 16) 8192\n",
      "50 (32, 16, 16) 8192\n",
      "51 (32, 16, 16) 8192\n",
      "52 (32, 16, 16) 8192\n",
      "53 (32, 16, 16) 8192\n",
      "54 (32, 16, 16) 8192\n",
      "55 (32, 16, 16) 8192\n",
      "56 (32, 16, 16) 8192\n",
      "57 (32, 32, 32) 32768\n",
      "58 (32, 32, 32) 32768\n",
      "59 (32, 32, 32) 32768\n",
      "60 (32, 32, 32) 32768\n",
      "61 (32, 32, 32) 32768\n",
      "62 (32, 32, 32) 32768\n",
      "63 (32, 32, 32) 32768\n",
      "64 (32, 32, 32) 32768\n",
      "65 (32, 32, 32) 32768\n",
      "66 (32, 32, 32) 32768\n",
      "67 (32, 32, 32) 32768\n",
      "68 (32, 32, 32) 32768\n",
      "69 (32, 32, 32) 32768\n",
      "70 (32, 32, 32) 32768\n",
      "71 (32, 32, 32) 32768\n",
      "72 (32, 32, 32) 32768\n",
      "73 (32, 64, 64) 131072\n",
      "74 (32, 64, 64) 131072\n",
      "75 (32, 64, 64) 131072\n",
      "76 (32, 64, 64) 131072\n",
      "77 (32, 64, 64) 131072\n",
      "78 (32, 64, 64) 131072\n",
      "79 (32, 64, 64) 131072\n",
      "80 (32, 64, 64) 131072\n",
      "81 (32, 64, 64) 131072\n",
      "82 (32, 64, 64) 131072\n",
      "83 (32, 64, 64) 131072\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(posterior_dist_list)):\n",
    "    print(i, posterior_dist_list[i][0][0].cpu().numpy().shape, posterior_dist_list[i][0][0].cpu().numpy().size)\n",
    "# posterior_dist_list[83][0].cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2213568"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 0\n",
    "for i in range(len(posterior_dist_list)):\n",
    "    embedding_size += posterior_dist_list[i][0][0].cpu().numpy().size\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(prior_kl_dist_list)):\n",
    "#     print(prior_kl_dist_list[i][0].cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 64, 64, 1, 2), (64, 64, 1, 2))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, v['image_1093444'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 22, 24, 25, 26, 27, 30, 31, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 79, 80, 81, 82])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 1, 2, 2) 4\n",
      "1 (1, 1, 1, 2) 2\n",
      "2 (1, 1, 1, 2) 2\n",
      "3 (1, 1, 2, 2) 4\n",
      "4 (1, 1, 1, 2) 2\n",
      "5 (1, 1, 2, 2) 4\n",
      "6 (4, 4, 2, 2) 64\n",
      "7 (4, 4, 2, 2) 64\n",
      "8 (4, 4, 2, 2) 64\n",
      "9 (4, 4, 2, 2) 64\n",
      "10 (4, 4, 2, 2) 64\n",
      "11 (4, 4, 2, 2) 64\n",
      "12 (4, 4, 1, 2) 32\n",
      "14 (8, 8, 1, 2) 128\n",
      "15 (8, 8, 1, 2) 128\n",
      "16 (8, 8, 2, 2) 256\n",
      "17 (8, 8, 2, 2) 256\n",
      "18 (8, 8, 2, 2) 256\n",
      "19 (8, 8, 2, 2) 256\n",
      "20 (8, 8, 1, 2) 128\n",
      "22 (8, 8, 1, 2) 128\n",
      "24 (8, 8, 1, 2) 128\n",
      "25 (8, 8, 1, 2) 128\n",
      "26 (8, 8, 1, 2) 128\n",
      "27 (8, 8, 1, 2) 128\n",
      "30 (8, 8, 1, 2) 128\n",
      "31 (8, 8, 1, 2) 128\n",
      "36 (16, 16, 1, 2) 512\n",
      "37 (16, 16, 1, 2) 512\n",
      "38 (16, 16, 1, 2) 512\n",
      "39 (16, 16, 1, 2) 512\n",
      "40 (16, 16, 1, 2) 512\n",
      "41 (16, 16, 1, 2) 512\n",
      "42 (16, 16, 1, 2) 512\n",
      "43 (16, 16, 1, 2) 512\n",
      "44 (16, 16, 1, 2) 512\n",
      "45 (16, 16, 1, 2) 512\n",
      "46 (16, 16, 1, 2) 512\n",
      "47 (16, 16, 1, 2) 512\n",
      "48 (16, 16, 1, 2) 512\n",
      "49 (16, 16, 1, 2) 512\n",
      "50 (16, 16, 1, 2) 512\n",
      "51 (16, 16, 1, 2) 512\n",
      "52 (16, 16, 1, 2) 512\n",
      "53 (16, 16, 1, 2) 512\n",
      "56 (16, 16, 1, 2) 512\n",
      "58 (32, 32, 1, 2) 2048\n",
      "59 (32, 32, 1, 2) 2048\n",
      "60 (32, 32, 1, 2) 2048\n",
      "61 (32, 32, 1, 2) 2048\n",
      "62 (32, 32, 1, 2) 2048\n",
      "63 (32, 32, 1, 2) 2048\n",
      "64 (32, 32, 1, 2) 2048\n",
      "65 (32, 32, 1, 2) 2048\n",
      "66 (32, 32, 1, 2) 2048\n",
      "67 (32, 32, 1, 2) 2048\n",
      "68 (32, 32, 1, 2) 2048\n",
      "69 (32, 32, 1, 2) 2048\n",
      "70 (32, 32, 1, 2) 2048\n",
      "71 (32, 32, 1, 2) 2048\n",
      "72 (32, 32, 1, 2) 2048\n",
      "74 (64, 64, 1, 2) 8192\n",
      "76 (64, 64, 1, 2) 8192\n",
      "77 (64, 64, 1, 2) 8192\n",
      "79 (64, 64, 1, 2) 8192\n",
      "80 (64, 64, 1, 2) 8192\n",
      "81 (64, 64, 1, 2) 8192\n",
      "82 (64, 64, 1, 2) 8192\n"
     ]
    }
   ],
   "source": [
    "for i in dist_dict.keys():\n",
    "    print(i, dist_dict[i]['image_1093444'].shape, dist_dict[i]['image_1093444'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 0\n",
    "for i in dist_dict.keys():\n",
    "    embedding_size += dist_dict[i]['image_1093444'].size\n",
    "    if i == 30:\n",
    "        break\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045415365599791827"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100530 / 2213568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (2, 32, 1, 1) 64 2 (2, 2, 1, 1) (1, 1, 2, 2) 4\n",
      "1 (2, 32, 1, 1) 64 1 (2, 1, 1, 1) (1, 1, 1, 2) 2\n",
      "2 (2, 32, 1, 1) 64 1 (2, 1, 1, 1) (1, 1, 1, 2) 2\n",
      "3 (2, 32, 1, 1) 64 2 (2, 2, 1, 1) (1, 1, 2, 2) 4\n",
      "4 (2, 32, 1, 1) 64 1 (2, 1, 1, 1) (1, 1, 1, 2) 2\n",
      "5 (2, 32, 1, 1) 64 2 (2, 2, 1, 1) (1, 1, 2, 2) 4\n",
      "6 (2, 32, 4, 4) 1024 2 (2, 2, 4, 4) (4, 4, 2, 2) 64\n",
      "7 (2, 32, 4, 4) 1024 2 (2, 2, 4, 4) (4, 4, 2, 2) 64\n",
      "8 (2, 32, 4, 4) 1024 2 (2, 2, 4, 4) (4, 4, 2, 2) 64\n",
      "9 (2, 32, 4, 4) 1024 2 (2, 2, 4, 4) (4, 4, 2, 2) 64\n",
      "10 (2, 32, 4, 4) 1024 2 (2, 2, 4, 4) (4, 4, 2, 2) 64\n",
      "11 (2, 32, 4, 4) 1024 2 (2, 2, 4, 4) (4, 4, 2, 2) 64\n",
      "12 (2, 32, 4, 4) 1024 1 (2, 1, 4, 4) (4, 4, 1, 2) 32\n",
      "13 (2, 32, 8, 8) 4096 No effective variates\n",
      "14 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "15 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "16 (2, 32, 8, 8) 4096 2 (2, 2, 8, 8) (8, 8, 2, 2) 256\n",
      "17 (2, 32, 8, 8) 4096 2 (2, 2, 8, 8) (8, 8, 2, 2) 256\n",
      "18 (2, 32, 8, 8) 4096 2 (2, 2, 8, 8) (8, 8, 2, 2) 256\n",
      "19 (2, 32, 8, 8) 4096 2 (2, 2, 8, 8) (8, 8, 2, 2) 256\n",
      "20 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "21 (2, 32, 8, 8) 4096 No effective variates\n",
      "22 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "23 (2, 32, 8, 8) 4096 No effective variates\n",
      "24 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "25 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "26 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "27 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "28 (2, 32, 8, 8) 4096 No effective variates\n",
      "29 (2, 32, 8, 8) 4096 No effective variates\n",
      "30 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "31 (2, 32, 8, 8) 4096 1 (2, 1, 8, 8) (8, 8, 1, 2) 128\n",
      "32 (2, 32, 16, 16) 16384 No effective variates\n",
      "33 (2, 32, 16, 16) 16384 No effective variates\n",
      "34 (2, 32, 16, 16) 16384 No effective variates\n",
      "35 (2, 32, 16, 16) 16384 No effective variates\n",
      "36 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "37 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "38 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "39 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "40 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "41 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "42 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "43 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "44 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "45 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "46 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "47 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "48 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "49 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "50 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "51 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "52 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "53 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "54 (2, 32, 16, 16) 16384 No effective variates\n",
      "55 (2, 32, 16, 16) 16384 No effective variates\n",
      "56 (2, 32, 16, 16) 16384 1 (2, 1, 16, 16) (16, 16, 1, 2) 512\n",
      "57 (2, 32, 32, 32) 65536 No effective variates\n",
      "58 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "59 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "60 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "61 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "62 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "63 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "64 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "65 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "66 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "67 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "68 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "69 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "70 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "71 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "72 (2, 32, 32, 32) 65536 1 (2, 1, 32, 32) (32, 32, 1, 2) 2048\n",
      "73 (2, 32, 64, 64) 262144 No effective variates\n",
      "74 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "75 (2, 32, 64, 64) 262144 No effective variates\n",
      "76 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "77 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "78 (2, 32, 64, 64) 262144 No effective variates\n",
      "79 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "80 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "81 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "82 (2, 32, 64, 64) 262144 1 (2, 1, 64, 64) (64, 64, 1, 2) 8192\n",
      "83 (2, 32, 64, 64) 262144 No effective variates\n"
     ]
    }
   ],
   "source": [
    "for i, (dist_list, variate_mask) in enumerate(zip(posterior_dist_list, variate_masks)):\n",
    "        # x = reshape_distribution(dist_list, variate_mask).detach().cpu().numpy()\n",
    "    dist = torch.stack(dist_list, dim=0)  # 2, batch_size, n_variates, H ,W\n",
    "    if variate_mask.any():\n",
    "        dist2 = dist[:, :, variate_mask, :, :]  # Only take effective variates\n",
    "        x = torch.permute(dist2, (1, 3, 4, 2, 0)).detach().cpu().numpy()  # batch_size, H ,W ,n_variates (subset), 2\n",
    "        # print(i, dist_list[0][0].cpu().numpy().shape, dist_list[0][0].cpu().numpy().size * 2, dist[:,0].detach().cpu().numpy().shape, x[0].shape, x[0].size)\n",
    "        print(i, dist[:,0].detach().cpu().numpy().shape, dist[:,0].detach().cpu().numpy().size, variate_mask.sum(), dist2[:,0].detach().cpu().numpy().shape, x[0].shape, x[0].size)\n",
    "    else:\n",
    "        print(i, dist[:,0].detach().cpu().numpy().shape, dist[:,0].detach().cpu().numpy().size, 'No effective variates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = torch.stack(dist_list, dim=0)  # 2, batch_size, n_variates, H ,W\n",
    "# len(dist_list), dist_list[0].shape, dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = dist[:, :, variate_mask, :, :]  # Only take effective variates\n",
    "# dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = torch.permute(dist, (1, 3, 4, 2, 0))  # batch_size, H ,W ,n_variates (subset), 2\n",
    "# dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(variate_masks)):\n",
    "#     print(i, variate_masks[i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = next(model.parameters()).device\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.ones((1, hparams.data.channels, hparams.data.target_res, hparams.data.target_res)).cuda(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
